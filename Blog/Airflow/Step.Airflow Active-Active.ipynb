{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5501b1b",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/d/de/AirflowLogo.png\" width=\"150\">\n",
    "\n",
    "## ✅ สร้าง Dags ในเครื่อง _Active 2_ โดย Task 1 ให้ใช้คำสั่ง Check การทำงานของเครื่อง _Active 1_ ดังนี้\n",
    "\n",
    "---\n",
    "__Usage Package ลงไว้ที่เครื่อง Active 2__\n",
    "* Ubuntu ```apt-get install -y netcat```\n",
    "* CentOS ```yum install -y nc```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b006f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkActive():\n",
    "    IPADDR = \"192.168.10.23\"\n",
    "    PORT = \"8080\"\n",
    "    check = os.system(f\"nc -zvw10 {IPADDR}:{PORT}\")\n",
    "    if check == 0:\n",
    "        raise AirflowSkipException\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e04ed1",
   "metadata": {},
   "source": [
    "## ✅ Run Function ```checkActive``` ใน Task แรก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479150ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = PythonOperator(\n",
    "    task_id='checkActive',\n",
    "    python_callable=checkActive,\n",
    "    dag=dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea718fa4",
   "metadata": {},
   "source": [
    "## ✅ Example Full code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99896858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import duckdb as db\n",
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.exceptions import AirflowSkipException\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark\n",
    "\n",
    "os.environ['HADOOP_CONF_DIR'] = '/etc/hadoop/conf'\n",
    "os.environ['JAVA_HOME'] = '/usr/local/jdk8u222-b10'\n",
    "os.environ['HADOOP_USER_NAME']='hive'\n",
    "os.environ['PYSPARK_PYTHON'] ='/HDFS01/anaconda3/envs/main/bin/python'\n",
    "conf = pyspark.SparkConf().setAll([\n",
    "     ('spark.driver.maxResultSize', '0'),\n",
    "     ('spark.driver.memory', '4g'),\n",
    "     ('spark.sql.repl.eagerEval.enabled','true'),\n",
    "     ('hive.strict.managed.tables','false'),\n",
    "     ('hive.metastore.uris', 'thrift://nn01.bigdata:9083'),\n",
    "     ('metastore.client.capability.check','false')\n",
    "    ])\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"myApp\") \\\n",
    "        .config(conf=conf) \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate();\n",
    "\n",
    "now = datetime.now()\n",
    "update_dt = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "today = now.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "#MemoryDb Connect\n",
    "mydb = db.connect(database=':memory:', read_only=False)\n",
    "\n",
    "args = {\n",
    "    'owner': 'Dags_name',\n",
    "    'depends_on_past': False,    \n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "def checkActive():\n",
    "    IPADDR = \"192.168.10.23\"\n",
    "    PORT = \"8080\"\n",
    "    check = os.system(f\"nc -zvw10 {IPADDR}:{PORT}\")\n",
    "    if check == 0:\n",
    "        raise AirflowSkipException\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "def getData():\n",
    "    mydb.sql(\"INSTALL httpfs;\")\n",
    "    mydb.sql(\"LOAD httpfs;\")\n",
    "    df = db.execute(\"\"\"SELECT\n",
    "                    column0 idx, wage, education, experience, ethnicity,\n",
    "                    CASE WHEN smsa = 'yes' THEN 'Yes' ELSE 'No' END AS smsa,\n",
    "                    CASE WHEN parttime = 'yes' THEN 'Yes' ELSE 'No' END AS parttime,\n",
    "                    CONCAT(UPPER(SUBSTRING(region, 1, 1)), SUBSTRING(region, 2)) AS region\n",
    "                FROM\n",
    "                    read_csv_auto\n",
    "                ('https://vincentarelbundock.github.io/Rdatasets/csv/AER/CPS1988.csv');\"\"\").df()\n",
    "    return df\n",
    "\n",
    "def toHdfs():\n",
    "    df = getData()\n",
    "    df2 = spark.createDataFrame(df)\n",
    "    if spark.sql('show tables in test_db') \\\n",
    "       .filter(\"tableName == 'nook'\").count() > 0:\n",
    "        df2.write \\\n",
    "            .mode('append') \\\n",
    "            .saveAsTable('test_db.nook')\n",
    "    else:\n",
    "        df2.write \\\n",
    "           .mode('overwrite') \\\n",
    "           .saveAsTable('test_db.nook')\n",
    "\n",
    "dag = DAG(\n",
    "    dag_id='Dags_name',\n",
    "    default_args=args,\n",
    "    start_date= datetime(2023, 5, 10),\n",
    "    description='Dags_name',\n",
    "    catchup=False,\n",
    "    schedule_interval='15 15 * * *',\n",
    ")\n",
    "t1 = PythonOperator(\n",
    "    task_id='checkActive',\n",
    "    python_callable=checkActive,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "t2 = PythonOperator(\n",
    "    task_id='getData',\n",
    "    python_callable=getData,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "t3 = PythonOperator(\n",
    "    task_id='toHdfs',\n",
    "    python_callable=toHdfs,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n",
    "t1 >> t2 >> t3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
